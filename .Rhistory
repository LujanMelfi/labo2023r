setwd("E:/MCD/labo/datasets") # establezco la carpeta donde voy a trabajar
# cargo el dataset
dataset <- fread("dataset_pequeno.csv")
dir.create("./exp/", showWarnings = FALSE)
dir.create("./exp/KA4800/", showWarnings = FALSE)
setwd("./exp/KA4800")
dataset[
foto_mes == 202107,
clase_binaria := ifelse(clase_ternaria == "BAJA+2", "POS", "NEG")
]
dataset_entrenar <- dataset[foto_mes == 202107]
dataset_aplicar <- dataset[foto_mes == 202109]
# elimino el campo clase ternaria de aplicar
dataset_aplicar[, clase_ternaria := NULL]
for (vmaxdepth in 4:25) {
# genero el modelo
modelo <- rpart(
formula = "clase_binaria ~ . ",
data = dataset_entrenar[, setdiff(
colnames(dataset_entrenar),
c("clase_ternaria")
), with = FALSE],
model = TRUE, # quiero que me devuelva el modelo
xval = 0,
cp = 0,
minsplit = 5,
maxdepth = vmaxdepth
)
# aplico el modelo a los datos en donde entrene
prediccion_202107 <- predict(
object = modelo,
newdata = dataset_entrenar,
type = "prob"
)
ganancia_202107 <- sum((prediccion_202107[, "POS"] > 0.025) *
ifelse(dataset_entrenar$clase_binaria == "POS", 117000, -3000))
cat(vmaxdepth, "\t", ganancia_202107, "\n")
prediccion_202109 <- predict(modelo,
dataset_aplicar,
type = "prob"
)
prob_pos <- prediccion_202109[, "POS"]
estimulo <- as.numeric(prob_pos > 0.025)
entrega <- as.data.table(list(
"numero_de_cliente" = dataset_aplicar$numero_de_cliente,
"Predicted" = estimulo
))
# genero el archivo para Kaggle
fwrite(entrega,
file = paste0("./altura_", vmaxdepth, ".csv")
)
}
library(readr)
HT3710 <- read_table("E:/MCD/labo/datasets/exp/HT3710/HT3710.txt")
View(HT3710)
library(readr)
HT3710 <- read_delim("E:/MCD/labo/datasets/exp/HT3710/HT3710.txt",
delim = "\t", escape_double = FALSE,
trim_ws = TRUE)
View(HT3710)
library(readr)
HT3710 <- read_delim("E:/MCD/labo/datasets/exp/HT3710/HT3710.txt",
delim = "\t", escape_double = FALSE,
trim_ws = TRUE)
maximo <- max(HT3710$)
maximo <- max(HT3710$ganancia)
maximo
max_param <- data.frame(cp = salida$cp[77,91]
, max_depth = salida$max_depth[77,91]
, min_split = salida$min_split[77,91]
, min_bucket = salida$min_bucket[77,91]
, ganancia_promedio = salida$ganancia_promedio[77,91])
max_param <- data.frame(cp = HT3710$cp[77,91]
, max_depth = HT3710$max_depth[77,91]
, min_split = HT3710$min_split[77,91]
, min_bucket = HT3710$min_bucket[77,91]
, ganancia_promedio = HT3710$ganancia_promedio[77,91])
max_param <- data.frame(cp = HT3710$cp[filas_deseadas],
max_depth = HT3710$max_depth[filas_deseadas],
min_split = HT3710$min_split[filas_deseadas],
min_bucket = HT3710$min_bucket[filas_deseadas],
ganancia_promedio = HT3710$ganancia[filas_deseadas])
filas_deseadas <- c(77, 91, 92, 94, 102, 107, 124, 125, 129, 130, 135, 139, 145, 146, 148, 152, 153, 154, 155, 156, 157, 158, 159, 161, 163, 164, 165, 166)
max_param <- data.frame(cp = HT3710$cp[filas_deseadas],
max_depth = HT3710$max_depth[filas_deseadas],
min_split = HT3710$min_split[filas_deseadas],
min_bucket = HT3710$min_bucket[filas_deseadas],
ganancia_promedio = HT3710$ganancia[filas_deseadas])
max_param <- data.frame(cp = HT3710$cp[filas_deseadas],
max_depth = HT3710$maxdepth[filas_deseadas],
min_split = HT3710$minsplit[filas_deseadas],
min_bucket = HT3710$minbucket[filas_deseadas],
ganancia_promedio = HT3710$ganancia[filas_deseadas])
View(max_param)
max_param <- data.frame(cp = HT3710$cp[filas_deseadas],
max_depth = HT3710$maxdepth[filas_deseadas],
min_split = HT3710$minsplit[filas_deseadas],
min_bucket = HT3710$minbucket[filas_deseadas],
ganancia_promedio = HT3710$ganancia[filas_deseadas])
max_param <- data.frame(cp = HT3710$cp[filas_deseadas],
max_depth = HT3710$maxdepth[filas_deseadas],
min_split = HT3710$minsplit[filas_deseadas],
min_bucket = HT3710$minbucket[filas_deseadas],
ganancia_promedio = HT3710$ganancia[filas_deseadas])
max_param <- data.frame(cp = HT3710$cp[filas_deseadas],
max_depth = HT3710$maxdepth[filas_deseadas],
min_split = HT3710$minsplit[filas_deseadas],
min_bucket = HT3710$minbucket[filas_deseadas],
ganancia_promedio = HT3710$ganancia[filas_deseadas])
library(openxlsx)
ruta_archivo <- "~/Desktop/max_param_371_BO_binaria.xlsx"
write.xlsx(max_param, ruta_archivo, row.names = FALSE)
write.xlsx(max_param, ruta_archivo, row.names = FALSE)
ruta_archivo <- "~/max_param_371_BO_binaria.xlsx"
write.xlsx(max_param, ruta_archivo, row.names = FALSE)
write.xlsx(max_param, ruta_archivo, rowNames = FALSE)
ruta_archivo <- "E:/MCD/labo/datasets/exp/HT3710/max_param_371_BO_binaria.xlsx"
write.xlsx(max_param, ruta_archivo, rowNames = FALSE)
# Ensemble de arboles de decision
# utilizando el naif metodo de Arboles Azarosos
# entreno cada arbol utilizando un subconjunto distinto de atributos del dataset
# limpio la memoria
rm(list = ls()) # Borro todos los objetos
gc() # Garbage Collection
require("data.table")
require("rpart")
# parmatros experimento
PARAM <- list()
PARAM$experimento <- 4
# Establezco la semilla aleatoria, cambiar por SU primer semilla
PARAM$semilla <- 980717
# parameetros rpart
PARAM$rpart_param <- list(
"cp" = -1,
"minsplit" = 250,
"minbucket" = 10,
"maxdepth" = 6
)
# parametros  arbol
# entreno cada arbol con solo 50% de las variables variables
PARAM$feature_fraction <- 0.5
# voy a generar 500 arboles, a mas arboles mas tiempo de proceso y MEJOR MODELO,
# pero ganancias marginales
PARAM$num_trees_max <- 500
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
# Aqui comienza el programa
setwd("E:/MCD/labo/datasets") # Establezco el Working Directory
# cargo los datos
#dataset <- fread("./datasets/dataset_pequeno.csv")
dataset <- fread("dataset_pequeno.csv")
# creo la carpeta donde va el experimento
dir.create("./exp/", showWarnings = FALSE)
carpeta_experimento <- paste0("./exp/KA", PARAM$experimento, "/")
dir.create(paste0("./exp/KA", PARAM$experimento, "/"),
showWarnings = FALSE
)
setwd(carpeta_experimento)
# que tamanos de ensemble grabo a disco, pero siempre debo generar los 500
grabar <- c(1, 5, 10, 50, 100, 200, 500)
# defino los dataset de entrenamiento y aplicacion
dtrain <- dataset[foto_mes == 202107]
dapply <- dataset[foto_mes == 202109]
# aqui se va acumulando la probabilidad del ensemble
dapply[, prob_acumulada := 0]
# Establezco cuales son los campos que puedo usar para la prediccion
# el copy() es por la Lazy Evaluation
campos_buenos <- copy(setdiff(colnames(dtrain), c("clase_ternaria")))
# Genero las salidas
set.seed(PARAM$semilla) # Establezco la semilla aleatoria
for (arbolito in 1:PARAM$num_trees_max) {
qty_campos_a_utilizar <- as.integer(length(campos_buenos)
* PARAM$feature_fraction)
campos_random <- sample(campos_buenos, qty_campos_a_utilizar)
# paso de un vector a un string con los elementos
# separados por un signo de "+"
# este hace falta para la formula
campos_random <- paste(campos_random, collapse = " + ")
# armo la formula para rpart
formulita <- paste0("clase_ternaria ~ ", campos_random)
# genero el arbol de decision
modelo <- rpart(formulita,
data = dtrain,
xval = 0,
control = PARAM$rpart_param
)
# aplico el modelo a los datos que no tienen clase
prediccion <- predict(modelo, dapply, type = "prob")
dapply[, prob_acumulada := prob_acumulada + prediccion[, "BAJA+2"]]
if (arbolito %in% grabar) {
# Genero la entrega para Kaggle
umbral_corte <- (1 / 40) * arbolito
entrega <- as.data.table(list(
"numero_de_cliente" = dapply[, numero_de_cliente],
"Predicted" = as.numeric(dapply[, prob_acumulada] > umbral_corte)
)) # genero la salida
nom_arch <- paste0(
"KA", PARAM$experimento, "_",
sprintf("%.3d", arbolito), # para que tenga ceros adelante
".csv"
)
fwrite(entrega,
file = nom_arch,
sep = ","
)
cat(arbolito, " ")
}
}
library(readr)
salida <- read_table("salida.txt")
# Gradient Boosting of Decision Trees
# Regresion
# Stumps   , arboles de dos hojas
# limpio la memoria
rm(list = ls(all.names = TRUE)) # remove all objects
gc(full = TRUE) # garbage collection
require("data.table")
PARAM <- list()
PARAM$learning_rate <- 0.3
PARAM$num_iterations <- 5
archivo <- "https://storage.googleapis.com/open-courses/austral2023r-e52a/AustralitosVirtualitos.txt"
#------------------------------------------------------------------------------
options(error = function() {
traceback(20)
options(error = NULL)
stop("exiting after script error")
})
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
# Aqui empieza el programa
# cargo el dataset
dataset <- fread(archivo)
# las variables independientes del dataset
campos_buenos <- setdiff(colnames(dataset), "Nota")
# el primer error es   ( Nota - promedio )
nota_mean <- dataset[, mean(Nota)]
dataset[, error := Nota - nota_mean]
# almaceno TODOS los splits de todos los arboles
tb_splits <- data.table(
iter = integer(),
campo = character(),
corte = numeric(),
gain = numeric(),
mejor = logical()
)
# almaceno el  rmse de los stumps
tb_stumps <- data.table(
iter = integer(),
rmse = numeric()
)
tb_stumps <- rbind(
tb_stumps,
list(0, dataset[, sqrt(mean(error * error))])
)
# avanzo por las iteraciones
for (iteracion in 1:PARAM$num_iterations) {
# recorro cada campo
for (campo in campos_buenos) {
# me quedo con < campo, error >
dcolumna <- dataset[
, list("error" = sum(error)),
list("valor" = get(campo))
]
# ordeno por los valores
setorder(dcolumna, valor)
# calculo los puntos de corte como el punto medio
dcolumna[, corte := frollmean(valor, 2, align = "left")]
# calculo el gain
dcolumna[, gain := cumsum(error)^2]
pos <- dcolumna[, which.max(abs(gain))]
reg <- dcolumna[pos]
# agrego el split a la tabla de splits
tb_splits <- rbind(
tb_splits,
list(
iteracion,
campo,
reg$corte,
abs(reg$gain),
FALSE
)
)
}
# busco el mejor split
tb_splits[iter == iteracion, mejor := .I == which.max(gain)]
reg <- tb_splits[iter == iteracion & mejor == TRUE]
# la hoja contribuye con LR * promedio
hoja1 <- dataset[get(reg$campo) < reg$corte, mean(error)]
dataset[get(reg$campo) < reg$corte, delta := -PARAM$learning_rate * hoja1]
hoja2 <- dataset[is.na(delta), mean(error)]
dataset[is.na(delta), delta := -PARAM$learning_rate * hoja2]
# modifico efectivamente el error = Boosting
dataset[, error := error + delta]
dataset[, delta := NULL]
tb_stumps <- rbind(
tb_stumps,
list(iteracion, dataset[, sqrt(mean(error * error))])
)
}
# imprimo
tb_stumps
tb_splits[mejor == TRUE]
library(readr)
HT4230 <- read_delim("E:/MCD/labo/datasets/exp/HT4230/HT4230.txt",
delim = "\t", escape_double = FALSE,
trim_ws = TRUE)
View(HT4230)
library(readr)
ht4230 <- read_table("HT4230.txt")
maximo <- max(HT4230$ganancia)
maximo
max_param <- data.frame(objective = HT4230$objective[56]
max_param <- data.frame(objective = HT4230$objective[56])
max_param <- data.frame(objective = HT4230$objective[56]
, metric = HT4230$metric[56]
, first_metric_only = HT4230$first_metric_only[56]
, boost_from_average = HT4230$boost_from_average[56]
, feature_pre_filter = HT4230$feature_pre_filter[56]
, verbosity = HT4230$verbosity[56]
, max_depth = HT4230$max_depth[56]
, min_gain_to_split = HT4230$min_gain_to_split[56]
, lambdal1 = HT4230$lambda_l1[56]
, lambdal2 = HT4230$lambda_l2[56]
, max_bin = HT4230$max_bin[56]
, num_iterations = HT4230$num_iterations[56]
, force_row_wise = HT4230$force_row_wise[56]
, seed = HT4230$seed[56]
, learing_rate = HT4230$learning_rate[56]
, feature_fraction = HT4230$feature_fraction[56]
, min_data_in_leaf = HT4230$min_data_in_leaf[56]
, num_leaves = HT4230$num_leaves[56]
, envios = HT4230$envios[56]
, ganancia = HT4230$ganancia[56]
, iteracion = HT4230$iteracion[56]
)
View(max_param)
# Par치metros de dicha ganancia m치xima
max_param <- data.frame(objective = HT4230$objective[59]
, metric = HT4230$metric[59]
, first_metric_only = HT4230$first_metric_only[59]
, boost_from_average = HT4230$boost_from_average[59]
, feature_pre_filter = HT4230$feature_pre_filter[59]
, verbosity = HT4230$verbosity[59]
, max_depth = HT4230$max_depth[59]
, min_gain_to_split = HT4230$min_gain_to_split[59]
, lambdal1 = HT4230$lambda_l1[59]
, lambdal2 = HT4230$lambda_l2[59]
, max_bin = HT4230$max_bin[59]
, num_iterations = HT4230$num_iterations[59]
, force_row_wise = HT4230$force_row_wise[59]
, seed = HT4230$seed[59]
, learing_rate = HT4230$learning_rate[59]
, feature_fraction = HT4230$feature_fraction[59]
, min_data_in_leaf = HT4230$min_data_in_leaf[59]
, num_leaves = HT4230$num_leaves[59]
, envios = HT4230$envios[59]
, ganancia = HT4230$ganancia[59]
, iteracion = HT4230$iteracion[59]
)
View(max_param)
View(HT4230)
segunda_max_param <- data.frame(objective = HT4230$objective[117]
, metric = HT4230$metric[117]
, first_metric_only = HT4230$first_metric_only[117]
, boost_from_average = HT4230$boost_from_average[117]
, feature_pre_filter = HT4230$feature_pre_filter[117]
, verbosity = HT4230$verbosity[117]
, max_depth = HT4230$max_depth[117]
, min_gain_to_split = HT4230$min_gain_to_split[117]
, lambdal1 = HT4230$lambda_l1[117]
, lambdal2 = HT4230$lambda_l2[117]
, max_bin = HT4230$max_bin[117]
, num_iterations = HT4230$num_iterations[117]
, force_row_wise = HT4230$force_row_wise[117]
, seed = HT4230$seed[117]
, learing_rate = HT4230$learning_rate[117]
, feature_fraction = HT4230$feature_fraction[117]
, min_data_in_leaf = HT4230$min_data_in_leaf[117]
, num_leaves = HT4230$num_leaves[117]
, envios = HT4230$envios[117]
, ganancia = HT4230$ganancia[117]
, iteracion = HT4230$iteracion[117]
)
max_param <- data.frame(objective = HT4230$objective[59]
, metric = HT4230$metric[59]
, first_metric_only = HT4230$first_metric_only[59]
, boost_from_average = HT4230$boost_from_average[59]
, feature_pre_filter = HT4230$feature_pre_filter[59]
, verbosity = HT4230$verbosity[59]
, max_depth = HT4230$max_depth[59]
, min_gain_to_split = HT4230$min_gain_to_split[59]
, lambdal1 = HT4230$lambda_l1[59]
, lambdal2 = HT4230$lambda_l2[59]
, max_bin = HT4230$max_bin[59]
, num_iterations = HT4230$num_iterations[59]
, force_row_wise = HT4230$force_row_wise[59]
, seed = HT4230$seed[59]
, learing_rate = HT4230$learning_rate[59]
, feature_fraction = HT4230$feature_fraction[59]
, min_data_in_leaf = HT4230$min_data_in_leaf[59]
, num_leaves = HT4230$num_leaves[59]
, envios = HT4230$envios[59]
, ganancia = HT4230$ganancia[59]
, iteracion = HT4230$iteracion[59]
)
cuarta_max_param <- data.frame(objective = HT4230$objective[95]
, metric = HT4230$metric[95]
, first_metric_only = HT4230$first_metric_only[95]
, boost_from_average = HT4230$boost_from_average[95]
, feature_pre_filter = HT4230$feature_pre_filter[95]
, verbosity = HT4230$verbosity[95]
, max_depth = HT4230$max_depth[95]
, min_gain_to_split = HT4230$min_gain_to_split[95]
, lambdal1 = HT4230$lambda_l1[95]
, lambdal2 = HT4230$lambda_l2[95]
, max_bin = HT4230$max_bin[95]
, num_iterations = HT4230$num_iterations[95]
, force_row_wise = HT4230$force_row_wise[95]
, seed = HT4230$seed[95]
, learing_rate = HT4230$learning_rate[95]
, feature_fraction = HT4230$feature_fraction[95]
, min_data_in_leaf = HT4230$min_data_in_leaf[95]
, num_leaves = HT4230$num_leaves[95]
, envios = HT4230$envios[95]
, ganancia = HT4230$ganancia[95]
, iteracion = HT4230$iteracion[95]
)
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
# Par치metros octava ganancia m치xima
octava_max_param <- data.frame(objective = HT4230$objective[116]
, metric = HT4230$metric[116]
, first_metric_only = HT4230$first_metric_only[116]
, boost_from_average = HT4230$boost_from_average[116]
, feature_pre_filter = HT4230$feature_pre_filter[116]
, verbosity = HT4230$verbosity[116]
, max_depth = HT4230$max_depth[116]
, min_gain_to_split = HT4230$min_gain_to_split[116]
, lambdal1 = HT4230$lambda_l1[116]
, lambdal2 = HT4230$lambda_l2[116]
, max_bin = HT4230$max_bin[116]
, num_iterations = HT4230$num_iterations[116]
, force_row_wise = HT4230$force_row_wise[116]
, seed = HT4230$seed[116]
, learing_rate = HT4230$learning_rate[116]
, feature_fraction = HT4230$feature_fraction[116]
, min_data_in_leaf = HT4230$min_data_in_leaf[116]
, num_leaves = HT4230$num_leaves[116]
, envios = HT4230$envios[116]
, ganancia = HT4230$ganancia[116]
, iteracion = HT4230$iteracion[116]
)
View(max_param)
View(segunda_max_param)
View(cuarta_max_param)
octava_max_param <- data.frame(objective = HT4230$objective[116]
, metric = HT4230$metric[116]
, first_metric_only = HT4230$first_metric_only[116]
, boost_from_average = HT4230$boost_from_average[116]
, feature_pre_filter = HT4230$feature_pre_filter[116]
, verbosity = HT4230$verbosity[116]
, max_depth = HT4230$max_depth[116]
, min_gain_to_split = HT4230$min_gain_to_split[116]
, lambdal1 = HT4230$lambda_l1[116]
, lambdal2 = HT4230$lambda_l2[116]
, max_bin = HT4230$max_bin[116]
, num_iterations = HT4230$num_iterations[116]
, force_row_wise = HT4230$force_row_wise[116]
, seed = HT4230$seed[116]
, learing_rate = HT4230$learning_rate[116]
, feature_fraction = HT4230$feature_fraction[116]
, min_data_in_leaf = HT4230$min_data_in_leaf[116]
, num_leaves = HT4230$num_leaves[116]
, envios = HT4230$envios[116]
, ganancia = HT4230$ganancia[116]
, iteracion = HT4230$iteracion[116]
)
View(octava_max_param)
View(cuarta_max_param)
View(segunda_max_param)
View(octava_max_param)
View(cuarta_max_param)
View(HT4230)
View(HT4230)
View(max_param)
library(readr)
dataset_pequeno <- read_csv("E:/MCD/labo/datasets/dataset_pequeno.csv")
View(dataset_pequeno)
library(readr)
ranger_BO <- read_delim("E:/MCD/labo/datasets/exp/HT330/ranger_BO.txt",
delim = "\t", escape_double = FALSE,
trim_ws = TRUE)
View(ranger_BO)
maximo <- max(ranger_BO$ganancia)
maximo
max_param <- data.frame(num.trees = ranger_BO$num.trees[34]
, max_depth = ranger_BO$max_depth[34]
, min.node.size = ranger_BO$min.node.size[34]
, mtry = ranger_BO$mtry[34]
, xval_folds = ranger_BO$xval_folds[34]
, ganancia = ranger_BO$ganancia[34]
, iteracion = ranger_BO$iteracion[34]
)
max_param <- data.frame(num.trees = ranger_BO$num.trees[34]
, max.depth = ranger_BO$max.depth[34]
, min.node.size = ranger_BO$min.node.size[34]
, mtry = ranger_BO$mtry[34]
, xval_folds = ranger_BO$xval_folds[34]
, ganancia = ranger_BO$ganancia[34]
, iteracion = ranger_BO$iteracion[34]
)
View(max_param)
